{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLAaIaAE9z4D"
      },
      "source": [
        "# Artificial Intelligence II - Homework 4\n",
        "# Question 3\n",
        "In this notebook we are going to evaluate how different datasets perform when we fine tune bert on one dataset and evaluate o another. In particular we will examine SQuAD and TriviaQA. \n",
        "\n",
        "*Note: Since I don't have access to GPUs and the datasets are large, I ran that notebook on kaggle and it is extremely slow to train so I could not do many experiments*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtso-sN-3Uhe"
      },
      "source": [
        "**Note**: You may need to change some paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-02T01:02:53.689179Z",
          "iopub.status.busy": "2022-03-02T01:02:53.688803Z",
          "iopub.status.idle": "2022-03-02T01:02:53.710568Z",
          "shell.execute_reply": "2022-03-02T01:02:53.709747Z",
          "shell.execute_reply.started": "2022-03-02T01:02:53.689085Z"
        },
        "id": "dWW0-97QvETZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-02T01:02:53.712783Z",
          "iopub.status.busy": "2022-03-02T01:02:53.712029Z",
          "iopub.status.idle": "2022-03-02T01:02:53.716848Z",
          "shell.execute_reply": "2022-03-02T01:02:53.715966Z",
          "shell.execute_reply.started": "2022-03-02T01:02:53.71274Z"
        },
        "id": "dDtBH8CFwHAt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# PATH = '/content/drive/MyDrive/Colab Notebooks/Artificial Intelligence II/bert/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g50JWXbB2ze"
      },
      "source": [
        "# Import Libraries and Read Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIUe14-z7O9q"
      },
      "source": [
        "Import libraries that will be used in this notebook, define a seeding function and set device to cuda if available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:09:50.354844Z",
          "iopub.status.busy": "2022-03-13T12:09:50.354574Z",
          "iopub.status.idle": "2022-03-13T12:09:58.370733Z",
          "shell.execute_reply": "2022-03-13T12:09:58.369916Z",
          "shell.execute_reply.started": "2022-03-13T12:09:50.354814Z"
        },
        "id": "A7j9F1YTMFuE",
        "outputId": "2df5236b-1df4-4ce0-e313-da4cab2579e5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
            "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\n",
            "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Working on: cuda\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from numpy import unravel_index\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sklearn.metrics\n",
        "import seaborn as sns\n",
        "import random\n",
        "import sys\n",
        "from IPython.display import Image\n",
        "import time\n",
        "\n",
        "# for text preprocessing\n",
        "import re\n",
        "import string\n",
        "\n",
        "!CUBLAS_WORKSPACE_CONFIG=:4096:2 # for cuda deterministic behavior\n",
        "\n",
        "######### BERT ############\n",
        "# first install transformers from hugging face\n",
        "!pip install transformers\n",
        "\n",
        "# imports\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "\n",
        "# dataloaders \n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "def set_seed(seed = 1234):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # torch.use_deterministic_algorithms(False)\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed()\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print('Working on:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMNGz5N3wNs5"
      },
      "source": [
        "Convert trivia QA dataset to SQuAD format. Using code from the paper \"What do Models Learn from Question Answering Datasets?\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T00:23:49.979113Z",
          "iopub.status.busy": "2022-03-13T00:23:49.978693Z",
          "iopub.status.idle": "2022-03-13T00:23:50.442141Z",
          "shell.execute_reply": "2022-03-13T00:23:50.44131Z",
          "shell.execute_reply.started": "2022-03-13T00:23:49.979066Z"
        },
        "id": "VH-QkMxfwNs5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
        "# You may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "# Script based on https://github.com/mandarjoshi90/triviaqa/blob/master/utils/convert_to_squad_format.py\n",
        "# We include functions that are modified from https://github.com/mandarjoshi90/triviaqa/tree/master/utils\n",
        "# cite: https://github.com/mandarjoshi90/triviaqa/\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import nltk\n",
        "# from utils.convert_to_squad_format import get_qad_triples\n",
        "def add_triple_data(datum, page, domain):\n",
        "    qad = {'Source': domain}\n",
        "    for key in ['QuestionId', 'Question', 'Answer']:\n",
        "        qad[key] = datum[key]\n",
        "    for key in page:\n",
        "        qad[key] = page[key]\n",
        "    return qad\n",
        "\n",
        "\n",
        "def get_qad_triples(data):\n",
        "    qad_triples = []\n",
        "    for datum in data['Data']:\n",
        "        for key in ['EntityPages', 'SearchResults']:\n",
        "            for page in datum.get(key, []):\n",
        "                qad = add_triple_data(datum, page, key)\n",
        "                qad_triples.append(qad)\n",
        "    return qad_triples\n",
        "\n",
        "# from utils.utils import get_file_contents\n",
        "\n",
        "def get_file_contents(filename, encoding='utf-8'):\n",
        "    with open(filename, encoding=encoding) as f:\n",
        "        content = f.read()\n",
        "    return content\n",
        "\n",
        "# from utils.dataset_utils import read_triviaqa_data, get_question_doc_string\n",
        "\n",
        "def read_clean_part(datum):\n",
        "    for key in ['EntityPages', 'SearchResults']:\n",
        "        new_page_list = []\n",
        "        for page in datum.get(key, []):\n",
        "            if page['DocPartOfVerifiedEval']:\n",
        "                new_page_list.append(page)\n",
        "        datum[key] = new_page_list\n",
        "    assert len(datum['EntityPages']) + len(datum['SearchResults']) > 0\n",
        "    return datum\n",
        "\n",
        "def read_json(filename, encoding='utf-8'):\n",
        "    contents = get_file_contents(filename, encoding=encoding)\n",
        "    return json.loads(contents)\n",
        "\n",
        "def read_triviaqa_data(qajson):\n",
        "    data = read_json(qajson)\n",
        "    # read only documents and questions that are a part of clean data set\n",
        "    if data['VerifiedEval']:\n",
        "        clean_data = []\n",
        "        for datum in data['Data']:\n",
        "            if datum['QuestionPartOfVerifiedEval']:\n",
        "                if data['Domain'] == 'Web':\n",
        "                    datum = read_clean_part(datum)\n",
        "                clean_data.append(datum)\n",
        "        data['Data'] = clean_data\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_question_doc_string(qid, doc_name):\n",
        "    return '{}--{}'.format(qid, doc_name)\n",
        "#-------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "def answer_index_in_document(answer, document):\n",
        "    answer_list = answer['Aliases'] + answer['NormalizedAliases']\n",
        "    for answer_string_in_doc in answer_list:\n",
        "        index = document.find(answer_string_in_doc)\n",
        "        if index != -1:\n",
        "            return answer_string_in_doc, index\n",
        "    return answer['NormalizedValue'], -1\n",
        "\n",
        "\n",
        "def select_relevant_portion(text):\n",
        "    paras = text.split('\\n')\n",
        "    selected = []\n",
        "    done = False\n",
        "    for para in paras:\n",
        "        sents = sent_tokenize.tokenize(para)\n",
        "        for sent in sents:\n",
        "            words = nltk.word_tokenize(sent)\n",
        "            for word in words:\n",
        "                selected.append(word)\n",
        "                if len(selected) >= 800:\n",
        "                    done = True\n",
        "                    break\n",
        "            if done:\n",
        "                break\n",
        "        if done:\n",
        "            break\n",
        "        selected.append('\\n')\n",
        "    st = ' '.join(selected).strip()\n",
        "    return st\n",
        "\n",
        "\n",
        "def triviaqa_to_squad_format(triviaqa_file, data_dir, output_file):\n",
        "    triviaqa_json = read_triviaqa_data(triviaqa_file)\n",
        "    qad_triples = get_qad_triples(triviaqa_json)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for triviaqa_example in qad_triples:\n",
        "        question_text = triviaqa_example['Question']\n",
        "        text = get_file_contents(os.path.join(data_dir, triviaqa_example['Filename']), encoding='utf-8')\n",
        "        context = select_relevant_portion(text)\n",
        "\n",
        "        para = {'context': context, 'qas': [{'question': question_text, 'answers': []}]}\n",
        "        data.append({'paragraphs': [para]})\n",
        "        qa = para['qas'][0]\n",
        "        qa['id'] = get_question_doc_string(triviaqa_example['QuestionId'], triviaqa_example['Filename'])\n",
        "        qa['is_impossible'] = True\n",
        "        ans_string, index = answer_index_in_document(triviaqa_example['Answer'], context)\n",
        "\n",
        "        if index != -1:\n",
        "            qa['answers'].append({'text': ans_string, 'answer_start': index})\n",
        "            qa['is_impossible'] = False\n",
        "\n",
        "    triviaqa_as_squad = {'data': data, 'version': '2.0'}\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        json.dump(triviaqa_as_squad, outfile, indent=2, sort_keys=True, ensure_ascii=False)\n",
        "\n",
        "\n",
        "\n",
        "sent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go4W1Nlewc9x"
      },
      "source": [
        "Get the trivia QA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-09T17:49:42.898949Z",
          "iopub.status.busy": "2022-03-09T17:49:42.89869Z",
          "iopub.status.idle": "2022-03-09T17:52:34.541044Z",
          "shell.execute_reply": "2022-03-09T17:52:34.540189Z",
          "shell.execute_reply.started": "2022-03-09T17:49:42.898922Z"
        },
        "id": "qTLnNsK8wNs7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!wget \"https://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2022-03-09T17:52:34.543653Z",
          "iopub.status.busy": "2022-03-09T17:52:34.543336Z",
          "iopub.status.idle": "2022-03-09T17:54:19.067453Z",
          "shell.execute_reply": "2022-03-09T17:54:19.066328Z",
          "shell.execute_reply.started": "2022-03-09T17:52:34.543614Z"
        },
        "id": "j2E3VTzLwNs7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!mkdir ./triviaqa \n",
        "!tar -C ./triviaqa -zxf triviaqa-rc.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98ymiH4QwjWI"
      },
      "source": [
        "Convert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-09T17:54:19.072744Z",
          "iopub.status.busy": "2022-03-09T17:54:19.072406Z",
          "iopub.status.idle": "2022-03-09T18:13:44.923154Z",
          "shell.execute_reply": "2022-03-09T18:13:44.922358Z",
          "shell.execute_reply.started": "2022-03-09T17:54:19.072702Z"
        },
        "id": "RzdjE6UawNs8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "triviaqa_to_squad_format('triviaqa/qa/wikipedia-train.json', 'triviaqa/evidence/wikipedia', 'triviaqa/triviaqa_train.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-09T18:13:44.930504Z",
          "iopub.status.busy": "2022-03-09T18:13:44.928434Z",
          "iopub.status.idle": "2022-03-09T18:16:16.031449Z",
          "shell.execute_reply": "2022-03-09T18:16:16.030684Z",
          "shell.execute_reply.started": "2022-03-09T18:13:44.930464Z"
        },
        "id": "335KTCj7wNs8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "triviaqa_to_squad_format('triviaqa/qa/wikipedia-dev.json', 'triviaqa/evidence/wikipedia', 'triviaqa/triviaqa_dev.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63U7HoojY7d4"
      },
      "source": [
        "Now we are going to preprocess the same way we did with SQuAD.\n",
        "\n",
        "I used the datasets library from hugging face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:09:58.373839Z",
          "iopub.status.busy": "2022-03-13T12:09:58.373617Z",
          "iopub.status.idle": "2022-03-13T12:10:06.588941Z",
          "shell.execute_reply": "2022-03-13T12:10:06.588124Z",
          "shell.execute_reply.started": "2022-03-13T12:09:58.373809Z"
        },
        "id": "CjtAYmNKY_Ew",
        "outputId": "6753d055-a1b8-46c2-cdc1-5a88b6627db1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.18.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.26.0)\n",
            "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.2.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.20.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.2)\n",
            "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:10:06.591048Z",
          "iopub.status.busy": "2022-03-13T12:10:06.590777Z",
          "iopub.status.idle": "2022-03-13T12:10:06.604571Z",
          "shell.execute_reply": "2022-03-13T12:10:06.603868Z",
          "shell.execute_reply.started": "2022-03-13T12:10:06.591008Z"
        },
        "id": "SXyJWICJwNs9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pyarrow as pa\n",
        "import json\n",
        "\n",
        "def jsontodataset(filepath, up_to=None):\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    ids = []\n",
        "    answers_list = []\n",
        "    count = 0\n",
        "    #     print(\"here\")\n",
        "    with open(filepath) as f:\n",
        "        squad = json.load(f)\n",
        "        for article in squad[\"data\"]:\n",
        "            title = article.get(\"title\", \"\").strip()\n",
        "            for paragraph in article[\"paragraphs\"]:\n",
        "                context = paragraph[\"context\"].strip()\n",
        "                for qa in paragraph[\"qas\"]:\n",
        "                    count+=1\n",
        "                    if up_to!=None and count>=up_to: \n",
        "                        dataset = datasets.Dataset(pa.Table.from_pydict({'context': contexts, 'question': questions, 'id': ids, 'answers': answers_list}))\n",
        "                        return dataset\n",
        "                    \n",
        "                    question = qa[\"question\"].strip()\n",
        "                    id_ = qa[\"id\"]\n",
        "\n",
        "                    answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
        "                    answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]\n",
        "\n",
        "\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    ids.append(id_)\n",
        "                    answers_list.append({\"answer_start\": answer_starts, \"text\": answers,})\n",
        "\n",
        "    dataset = datasets.Dataset(pa.Table.from_pydict({'context': contexts, 'question': questions, 'id': ids, 'answers': answers_list}))\n",
        "    #     print(dataset)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:44:54.228254Z",
          "iopub.status.busy": "2022-03-13T12:44:54.227615Z",
          "iopub.status.idle": "2022-03-13T12:45:04.667263Z",
          "shell.execute_reply": "2022-03-13T12:45:04.666471Z",
          "shell.execute_reply.started": "2022-03-13T12:44:54.228215Z"
        },
        "id": "qdf-2pg8wNs9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# train_dataset = jsontodataset('./triviaqa/triviaqa_train.json')\n",
        "train_dataset = jsontodataset('../input/trivia-squadformat/triviaqa_train (1).json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:45:04.669535Z",
          "iopub.status.busy": "2022-03-13T12:45:04.669275Z",
          "iopub.status.idle": "2022-03-13T12:45:06.731972Z",
          "shell.execute_reply": "2022-03-13T12:45:06.731200Z",
          "shell.execute_reply.started": "2022-03-13T12:45:04.669499Z"
        },
        "id": "eY53eCK2wNs-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# train_dataset = jsontodataset('./triviaqa/triviaqa_dev.json')\n",
        "validation_dataset = jsontodataset('../input/trivia-squadformat/triviaqa_dev (1).json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6lJFPi7wNs-"
      },
      "source": [
        "Overview of the feature names of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:45:34.692694Z",
          "iopub.status.busy": "2022-03-13T12:45:34.692108Z",
          "iopub.status.idle": "2022-03-13T12:45:34.698889Z",
          "shell.execute_reply": "2022-03-13T12:45:34.698104Z",
          "shell.execute_reply.started": "2022-03-13T12:45:34.692652Z"
        },
        "id": "3H4KdhQ9wNs-",
        "outputId": "1a2bf3cc-c06d-4b96-fa95-f85f29b41f42",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['context', 'question', 'id', 'answers'],\n",
              "    num_rows: 110647\n",
              "})"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4diPoKBwNs-"
      },
      "source": [
        "Let's print the first example.\n",
        "\n",
        "We see that for 'answers' column the dataset contains a dictionary with keys 'text' and 'answer_start', that each contain a list with one element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:45:38.066974Z",
          "iopub.status.busy": "2022-03-13T12:45:38.066444Z",
          "iopub.status.idle": "2022-03-13T12:45:38.073346Z",
          "shell.execute_reply": "2022-03-13T12:45:38.072374Z",
          "shell.execute_reply.started": "2022-03-13T12:45:38.066913Z"
        },
        "id": "-6xqau1nwNs-",
        "outputId": "76c6572b-dc10-4220-e7e4-e9d3419602f0",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': \"England is a country that is part of the United Kingdom . It shares land borders with Scotland to the north and Wales to the west . The Irish Sea lies northwest of England and the Celtic Sea lies to the southwest . England is separated from continental Europe by the North Sea to the east and the English Channel to the south . The country covers much of the central and southern part of the island of Great Britain , which lies in the North Atlantic ; and includes over 100 smaller islands such as the Isles of Scilly , and the Isle of Wight . \\n \\n The area now called England was first inhabited by modern humans during the Upper Palaeolithic period , but takes its name from the Angles , one of the Germanic tribes who settled during the 5th and 6th centuries . England became a unified state in the 10th century , and since the Age of Discovery , which began during the 15th century , has had a significant cultural and legal impact on the wider world . The English language , the Anglican Church , and English law – the basis for the common law legal systems of many other countries around the world – developed in England , and the country 's parliamentary system of government has been widely adopted by other nations . The Industrial Revolution began in 18th-century England , transforming its society into the world 's first industrialised nation . \\n \\n England 's terrain mostly comprises low hills and plains , especially in central and southern England . However , there are uplands in the north ( for example , the mountainous Lake District , Pennines , and Yorkshire Dales ) and in the south west ( for example , Dartmoor and the Cotswolds ) . The capital is London , which is the largest metropolitan area in both the United Kingdom and the European Union.According to the European Statistical Agency , London is the largest Larger Urban Zone in the EU , a measure of metropolitan area which comprises a city 's urban core as well as its surrounding commuting zone . London 's municipal population is also the largest in the EU . England 's population of over 53 million comprises 84 % of the population of the United Kingdom , largely concentrated around London , the South East , and conurbations in the Midlands , the North West , the North East , and Yorkshire , which each developed as major industrial regions during the 19th century . [ http : //www.ons.gov.uk/ons/dcp171778_270487.pdf 2011 Census – Population and household estimates for England and Wales , March 2011 ] . Accessed 31 May 2013 . \\n \\n The Kingdom of England—which after 1535 included Wales—ceased being a separate sovereign state on 1 May 1707 , when the Acts of Union put into effect the terms agreed in the Treaty of Union the previous year , resulting in a political union with the Kingdom of Scotland to create the Kingdom of Great Britain . In 1801 , Great Britain was united with the Kingdom of Ireland through another Act of Union to become the United Kingdom of Great Britain and Ireland . In 1922 the Irish Free State seceded from the United Kingdom , leading to the latter being renamed the United Kingdom of Great Britain and Northern Ireland . \\n \\n Toponymy \\n \\n The name `` England '' is derived from the Old English name Englaland , which means `` land of the Angles '' . The Angles were one of the Germanic tribes that settled in Great Britain during the Early Middle Ages . The Angles came from the Angeln peninsula in the Bay of Kiel area of the Baltic Sea . The earliest recorded use of the term , as `` Engla londe '' , is in the late ninth century translation into Old English of Bede 's Ecclesiastical History of the English People . The term was then used in a different sense to the modern one , meaning `` the land inhabited by the English '' , and it included English people in what is now south-east Scotland but was then part of the English kingdom of Northumbria . The Anglo-Saxon Chronicle recorded that the Domesday Book of 1086 covered the whole of England , meaning the English kingdom , but a few years later the Chronicle stated that King Malcolm III went `` out of Scotlande into Lothian in Englaland '' , thus using it in the more ancient sense . According to the Oxford English Dictionary , its modern spelling was first used in 1538 . \\n \\n The earliest attested reference to the Angles occurs in the 1st-century\",\n",
              " 'question': 'Where in England was Dame Judi Dench born?',\n",
              " 'id': 'tc_3--England.txt',\n",
              " 'answers': {'answer_end': 1573, 'answer_start': 1569, 'text': 'York'}}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkZL7XQJwNs_"
      },
      "source": [
        "Same features for the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T15:54:11.659703Z",
          "iopub.status.busy": "2022-03-13T15:54:11.658867Z",
          "iopub.status.idle": "2022-03-13T15:54:11.665204Z",
          "shell.execute_reply": "2022-03-13T15:54:11.664471Z",
          "shell.execute_reply.started": "2022-03-13T15:54:11.659655Z"
        },
        "id": "eVVrUwynQGbg",
        "outputId": "2d6c704d-d357-43bd-995e-0c0bf305e377",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['context', 'question', 'id', 'answers', 'input_ids', 'attention_mask', 'offset_mapping'],\n",
              "    num_rows: 14229\n",
              "})"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_6_2Eg9-RU0"
      },
      "source": [
        "In this exercise I will use distilbert instead of bert because this dataset is larger and distilbert according to documentation is faster. \n",
        "> We leverage knowledge distillation during the pre-training phase and show\n",
        "that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:10:30.827624Z",
          "iopub.status.busy": "2022-03-13T12:10:30.827377Z",
          "iopub.status.idle": "2022-03-13T12:10:33.254768Z",
          "shell.execute_reply": "2022-03-13T12:10:33.254008Z",
          "shell.execute_reply.started": "2022-03-13T12:10:30.827588Z"
        },
        "id": "nIUEr1Rq-RU6",
        "outputId": "e85db52c-15b1-4ee5-af17-762dd754df7a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQbQUW6v-RU7"
      },
      "source": [
        "Find encodings for BERT with tokenizer. The max length of a sequence (question+anwer with special tokens) is 489, so I use that number for padding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauZRzlKhglZ"
      },
      "source": [
        "# Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UV7TAocwNtA"
      },
      "source": [
        "For the training dataset, I noticed that for each question there is only one answer, so there is no need to keep the values of the answers dictionary in lists. For example: `'answers': {'text': ['singing and dancing'], 'answer_start': [207]}}` can be reformated to `'answers': {'text': 'singing and dancing', 'answer_start': 207}}`. As for questions that are inanswerable (they look like this:`'answers': {'text': [], 'answer_start': []}}` we can just have `'answers': {'text': \"\", 'answer_start': 0}}`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI3lwp_YwNtB"
      },
      "source": [
        "Let's find the end character index that we will use to find the end token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f29fcd95f7c7432aafd239d7a98fa20e"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-13T12:45:17.181074Z",
          "iopub.status.busy": "2022-03-13T12:45:17.180518Z",
          "iopub.status.idle": "2022-03-13T12:45:33.272481Z",
          "shell.execute_reply": "2022-03-13T12:45:33.271782Z",
          "shell.execute_reply.started": "2022-03-13T12:45:17.181032Z"
        },
        "id": "yUYINaYfwNtB",
        "outputId": "9ab5b854-4780-4f7f-86ca-fde1b6eec016",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f29fcd95f7c7432aafd239d7a98fa20e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0ex [00:00, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def find_end(example):\n",
        "\n",
        "    if (len(example['answers']['text']) != 0):\n",
        "        context = example['context']\n",
        "        text = example['answers']['text'][0]\n",
        "        start_idx = example['answers']['answer_start'][0]\n",
        "\n",
        "        end_idx = start_idx + len(text)\n",
        "        \n",
        "        temp = example['answers'] # to change the value\n",
        "        temp['answer_end']=end_idx \n",
        "        temp['answer_start'] = start_idx # [num]->num\n",
        "        temp['text'] = text # ['text']->text\n",
        "    \n",
        "    else:\n",
        "        temp = example['answers']\n",
        "        temp['answer_end'] = 0 # []->0\n",
        "        temp['answer_start'] = 0 # []->0\n",
        "        temp['text'] = \"\" # []->\"\"\n",
        "        \n",
        "    return example\n",
        "\n",
        "train_dataset = train_dataset.map(find_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkQbAYFJwNtB"
      },
      "source": [
        "Check some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:46:28.124163Z",
          "iopub.status.busy": "2022-03-13T12:46:28.123321Z",
          "iopub.status.idle": "2022-03-13T12:46:28.131524Z",
          "shell.execute_reply": "2022-03-13T12:46:28.130485Z",
          "shell.execute_reply.started": "2022-03-13T12:46:28.124117Z"
        },
        "id": "-7DwUP6rwNtB",
        "outputId": "993aec58-d644-48e5-c15b-ebc856be72a9",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': \"Dame Judith Olivia `` Judi '' Dench , ( born 9 December 1934 ) is an English actress and author . Dench made her professional debut in 1957 with the Old Vic Company . Over the following few years she performed in several of Shakespeare 's plays in such roles as Ophelia in Hamlet , Juliet in Romeo and Juliet and Lady Macbeth in Macbeth . Although most of her work during this period was in theatre , she also branched into film work , and won a BAFTA Award as Most Promising Newcomer . She drew strong reviews for her leading role in the musical Cabaret in 1968 . \\n \\n Over the next two decades , Dench established herself as one of the most significant British theatre performers , working for the National Theatre Company and the Royal Shakespeare Company . She achieved success in television during this period , in the series A Fine Romance from 1981 until 1984 , and in 1992 with a starring role in the romantic comedy series As Time Goes By . Her film appearances were infrequent and included supporting roles in major films such as A Room with a View ( 1986 ) supporting Maggie Smith , before she rose to international fame as M in GoldenEye ( 1995 ) , a role she continued to play in James Bond films until Spectre ( 2015 ) . She received her first Oscar nomination for Best Actress for her role as Queen Victoria in Mrs Brown ( 1997 ) and the following year won the Academy Award for Best Supporting Actress for Shakespeare in Love . A seven-time Oscar nominee , she has also received nominations for her roles in Chocolat ( 2000 ) , Iris ( 2001 ) , Mrs Henderson Presents ( 2005 ) , Notes on a Scandal ( 2006 ) , and Philomena ( 2013 ) . \\n \\n Dench has received many award nominations for her acting in theatre , film and television ; her competitive awards include six British Academy Film Awards , four BAFTA TV Awards , seven Olivier Awards , two Screen Actors Guild Awards , two Golden Globes , an Academy Award , and a Tony Award . She has also received the BAFTA Fellowship ( 2001 ) and the Special Olivier Award ( 2004 ) . In June 2011 , she received a fellowship from the British Film Institute ( BFI ) . Dench is also a Fellow of the Royal Society of Arts ( FRSA ) . \\n \\n Early life \\n \\n Dench was born in Heworth , North Riding of Yorkshire . Her mother , Eleanora Olive ( née Jones ) , was born in Dublin . Her father , Reginald Arthur Dench , a doctor , was born in Dorset , and later moved to Dublin , where he was raised . He met Dench 's mother while he was studying medicine at Trinity College , Dublin . \\n \\n Dench attended The Mount School , a Quaker independent secondary school in York , and became a Quaker . Her brothers , one of whom was actor Jeffery Dench , were born in Tyldesley , Lancashire . Her niece , Emma Dench , is a Roman historian and professor previously at Birkbeck , University of London , and currently at Harvard University . \\n \\n Career \\n \\n In Britain , Dench has developed a reputation as one of the greatest actresses of the post-war period , primarily through her work in theatre , which has been her forte throughout her career . She has more than once been named number one in polls for Britain 's best actor . \\n \\n Early years \\n \\n Through her parents , Dench had regular contact with the theatre . Her father , a physician , was also the GP for the York theatre , and her mother was its wardrobe mistress . Actors often stayed in the Dench household . During these years , Judi Dench was involved on a non-professional basis in the first three productions of the modern revival of the York Mystery Plays in the 1950s . In 1957 , in one of the last productions in which she appeared during this period , she played the role of the Virgin Mary , performed on a fixed stage in the Museum Gardens . Though she initially trained as a set designer , she became interested in drama school as her brother Jeff attended the Central School of Speech and Drama . She applied and was accepted , where she was a classmate of Vanessa Redgrave , graduating with a first class degree in drama and four acting prizes , one being the Gold Medal as Outstanding\",\n",
              " 'question': 'Where in England was Dame Judi Dench born?',\n",
              " 'id': 'tc_3--Judi_Dench.txt',\n",
              " 'answers': {'answer_end': 2252, 'answer_start': 2248, 'text': 'York'}}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGNY6xxVwNtB"
      },
      "source": [
        "Tokenize train dataset and find end and start tokens. The sequence lenght will be 512, the maximum one for bert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a782044811f64e50b5d8c1e59a184f19"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-13T12:46:30.396216Z",
          "iopub.status.busy": "2022-03-13T12:46:30.395644Z",
          "iopub.status.idle": "2022-03-13T12:50:58.872674Z",
          "shell.execute_reply": "2022-03-13T12:50:58.871891Z",
          "shell.execute_reply.started": "2022-03-13T12:46:30.396174Z"
        },
        "id": "U8LqkNe8wNtB",
        "outputId": "c38b38df-a2a6-402f-e168-6192a1d4ad8d",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a782044811f64e50b5d8c1e59a184f19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/111 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def encode(examples):\n",
        "    tokenized = tokenizer(examples['context'], examples['question'], truncation=True, padding=True)\n",
        "    start_token_list = []\n",
        "    end_token_list = []\n",
        "    answers = examples['answers']\n",
        "    for i in range(len(answers)):\n",
        "        if (answers[i]['text'] != ''):\n",
        "            start_token = tokenized.char_to_token(i, answers[i]['answer_start'])\n",
        "            end_token = tokenized.char_to_token(i, answers[i]['answer_end'] - 1)\n",
        "            \n",
        "            # if start token is None, the answer passage has been truncated\n",
        "            if start_token is None:\n",
        "                start_token = tokenizer.model_max_length\n",
        "            if end_token is None:\n",
        "                end_token = tokenizer.model_max_length\n",
        "        else:\n",
        "            start_token = 0\n",
        "            end_token = 0\n",
        "            \n",
        "        start_token_list.append(start_token)\n",
        "        end_token_list.append(end_token)\n",
        "            \n",
        "\n",
        "    return {\"start_position\": start_token_list, \"end_position\": end_token_list, \"input_ids\": tokenized['input_ids'], \"attention_mask\": tokenized['attention_mask']}\n",
        "\n",
        "train_dataset = train_dataset.map(encode, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_plM9dS4wNtC"
      },
      "source": [
        "The train dataset is now updated with the columns 'start_position', 'end_position', 'input_ids' and 'attention_mask'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:50:58.883974Z",
          "iopub.status.busy": "2022-03-13T12:50:58.883556Z",
          "iopub.status.idle": "2022-03-13T12:50:58.891461Z",
          "shell.execute_reply": "2022-03-13T12:50:58.890695Z",
          "shell.execute_reply.started": "2022-03-13T12:50:58.883915Z"
        },
        "id": "9Sx1Odc4wNtC",
        "outputId": "a3b64e59-82d6-4761-f416-f5955de50f86",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['context', 'question', 'id', 'answers'],\n",
              "    num_rows: 110647\n",
              "})"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:52:59.944034Z",
          "iopub.status.busy": "2022-03-13T12:52:59.943462Z",
          "iopub.status.idle": "2022-03-13T12:54:04.799220Z",
          "shell.execute_reply": "2022-03-13T12:54:04.798410Z",
          "shell.execute_reply.started": "2022-03-13T12:52:59.943992Z"
        },
        "id": "0e4Pv5Kshgla",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "train_data = TensorDataset(torch.tensor(train_dataset['input_ids'], dtype=torch.int64), \n",
        "                           torch.tensor(train_dataset['attention_mask'], dtype=torch.float), \n",
        "                           torch.tensor(train_dataset['start_position'], dtype=torch.int64), \n",
        "                           torch.tensor(train_dataset['start_position'], dtype=torch.int64))\n",
        "\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTI7xyilwNtC"
      },
      "source": [
        "Validation dataset does not need that much preprocessing. I pass to the dataloader only the input_ids and attention masks, that will be passed to distilbert model in batches. I use a Sequential sampler to keep the indexing same as the validation dataset. We will need the offsets mapping to construct the sentence from the predicted start and end tokens and compare it with the actual answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "68cc5b11131a470eb218f10e9366275a"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-13T15:44:25.853731Z",
          "iopub.status.busy": "2022-03-13T15:44:25.852829Z",
          "iopub.status.idle": "2022-03-13T15:47:15.538242Z",
          "shell.execute_reply": "2022-03-13T15:47:15.537460Z",
          "shell.execute_reply.started": "2022-03-13T15:44:25.853686Z"
        },
        "id": "7sdTqae8wNtC",
        "outputId": "58561ac8-8390-4317-b0a7-9bb31a8ba9bb",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68cc5b11131a470eb218f10e9366275a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "validation_dataset = validation_dataset.map(lambda examples: tokenizer(examples['context'], examples['question'], truncation=True, padding=True, return_offsets_mapping=True), batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T15:47:15.540270Z",
          "iopub.status.busy": "2022-03-13T15:47:15.539999Z",
          "iopub.status.idle": "2022-03-13T15:47:23.443014Z",
          "shell.execute_reply": "2022-03-13T15:47:23.442257Z",
          "shell.execute_reply.started": "2022-03-13T15:47:15.540233Z"
        },
        "id": "0iUqLqCJwNtC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "val_data = TensorDataset(torch.tensor(validation_dataset['input_ids'], dtype=torch.int64), \n",
        "                        torch.tensor(validation_dataset['attention_mask'], dtype=torch.float))\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MReVWUdjhgla"
      },
      "source": [
        "# Fine-Tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PLbVIPPwNtD"
      },
      "source": [
        "The training for each epoch took aprox. 1 hour so I couldn't try many epochs and do many runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhI0ASjkwNtD"
      },
      "source": [
        "For optimizer, I used AdamW (Adam with weight decay) which is the one that was used in BERT during pre-training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:54:36.267614Z",
          "iopub.status.busy": "2022-03-13T12:54:36.267133Z",
          "iopub.status.idle": "2022-03-13T12:54:38.583743Z",
          "shell.execute_reply": "2022-03-13T12:54:38.582979Z",
          "shell.execute_reply.started": "2022-03-13T12:54:36.267576Z"
        },
        "id": "4ezESoUuhgla",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "model.to(device)\n",
        "optimizer = optim.AdamW(model\n",
        "                        .parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:54:38.585763Z",
          "iopub.status.busy": "2022-03-13T12:54:38.585497Z",
          "iopub.status.idle": "2022-03-13T15:39:29.425262Z",
          "shell.execute_reply": "2022-03-13T15:39:29.424442Z",
          "shell.execute_reply.started": "2022-03-13T12:54:38.585725Z"
        },
        "id": "b0Cp_bo4QV_S",
        "outputId": "c1b55f94-6519-47f1-df5a-18e2ed33b7c4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 13831/13831 [54:54<00:00,  4.20it/s, Loss=2.1] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 2.1038754528256267\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 13831/13831 [55:04<00:00,  4.19it/s, Loss=1.37]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 1.3681465321896578\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 13831/13831 [54:49<00:00,  4.20it/s, Loss=1.03]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 1.0256071441891226\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "# model.load_state_dict(torch.load(\"./weights_\" + str(0) + \".pth\"))\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = []\n",
        "    validation_loss = []\n",
        "    \n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    count=-1\n",
        "    progress_bar = tqdm(train_dataloader, leave=True, position=0)\n",
        "    progress_bar.set_description(f\"Epoch {epoch+1}\")\n",
        "    for batch in progress_bar:\n",
        "        count+=1\n",
        "        input_ids, mask, start, end  = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss, start_logits, end_logits = model(input_ids = input_ids, \n",
        "                                                attention_mask = mask, \n",
        "                                                start_positions = start, \n",
        "                                                end_positions = end,\n",
        "                                                return_dict = False)           \n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (count % 20 == 0 and count != 0):\n",
        "            avg = total_loss/count\n",
        "            progress_bar.set_postfix(Loss=avg)\n",
        "            \n",
        "    torch.save(model.state_dict(), \"./trivia\" + str(epoch) + \".h5\") # save for later use\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    epoch_loss.append(avg_train_loss)\n",
        "    print(f\"Epoch {epoch} Loss: {avg_train_loss}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do-qX2pEgCMU"
      },
      "source": [
        "# Evaluate trivia qa on trivia qa\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T15:57:19.508519Z",
          "iopub.status.busy": "2022-03-13T15:57:19.508014Z",
          "iopub.status.idle": "2022-03-13T16:29:46.700632Z",
          "shell.execute_reply": "2022-03-13T16:29:46.699754Z",
          "shell.execute_reply.started": "2022-03-13T15:57:19.508485Z"
        },
        "id": "PRFIYVtdQ07R",
        "outputId": "bcac5809-99fe-4cae-dbae-ea6cce76a1a7",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1779/1779 [32:26<00:00,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy is:  0.3723381825848619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "model.load_state_dict(torch.load(\"./trivia2.h5\"))\n",
        "model.to(device)\n",
        "\n",
        "threshold = 1.0\n",
        "epoch_i = 0\n",
        "correct = 0 \n",
        "pred_dict = {}\n",
        "na_prob_dict = {}\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "batch_val_losses = []\n",
        "row = 0\n",
        "for test_batch in tqdm(val_dataloader):\n",
        "    input_ids, masks = tuple(t.to(device) for t in test_batch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # prediction logits\n",
        "        start_logits, end_logits = model(input_ids=input_ids,\n",
        "                                        attention_mask=masks,\n",
        "                                        return_dict=False)\n",
        "\n",
        "    # to cpu\n",
        "    start_logits = start_logits.detach().cpu()\n",
        "    end_logits = end_logits.detach().cpu()\n",
        "\n",
        "    # for every sequence in batch \n",
        "    for bidx in range(len(start_logits)):\n",
        "        # apply softmax to logits to get scores\n",
        "        start_scores = np.array(F.softmax(start_logits[bidx], dim = 0))\n",
        "        end_scores = np.array(F.softmax(end_logits[bidx], dim = 0))\n",
        "\n",
        "        # find max for start<=end\n",
        "        size = len(start_scores)\n",
        "        scores = np.zeros((size, size))\n",
        "        for j in range(size):\n",
        "            for i in range(j+1): # include j\n",
        "                scores[i,j] = start_scores[i] + end_scores[j]\n",
        "\n",
        "        # find best i and j\n",
        "        start_pred, end_pred = unravel_index(scores.argmax(), scores.shape)\n",
        "        answer_pred = \"\"\n",
        "        if (scores[start_pred, end_pred] > scores[0,0]+threshold):\n",
        "\n",
        "            offsets = validation_dataset[row]['offset_mapping']\n",
        "            pred_char_start = offsets[start_pred][0]\n",
        "\n",
        "            if end_pred < len(offsets):\n",
        "                pred_char_end = offsets[end_pred][1]\n",
        "                answer_pred = validation_dataset[row]['context'][pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                answer_pred = validation_dataset[row]['context'][pred_char_start:]\n",
        "                \n",
        "            if answer_pred in validation_dataset[row]['answers']['text']:\n",
        "                correct += 1\n",
        "\n",
        "        else:\n",
        "            if (len(validation_dataset[row]['answers']['text']) ==0):\n",
        "                correct += 1        \n",
        "\n",
        "        pred_dict[validation_dataset[row]['id']] = answer_pred\n",
        "        na_prob_dict[validation_dataset[row]['id']] = scores[0,0]\n",
        "\n",
        "        row+=1\n",
        "\n",
        "\n",
        "accuracy = correct/validation_dataset.num_rows\n",
        "print(\"accuracy is: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T16:42:19.717029Z",
          "iopub.status.busy": "2022-03-13T16:42:19.716740Z",
          "iopub.status.idle": "2022-03-13T16:42:19.750823Z",
          "shell.execute_reply": "2022-03-13T16:42:19.750070Z",
          "shell.execute_reply.started": "2022-03-13T16:42:19.716996Z"
        },
        "id": "Grqyte04wNtE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json \n",
        "with open(\"pred.json\", \"w\") as outfile:\n",
        "    json.dump(pred_dict, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T16:42:21.010140Z",
          "iopub.status.busy": "2022-03-13T16:42:21.009708Z",
          "iopub.status.idle": "2022-03-13T16:42:21.065932Z",
          "shell.execute_reply": "2022-03-13T16:42:21.065141Z",
          "shell.execute_reply.started": "2022-03-13T16:42:21.010101Z"
        },
        "id": "gNjnV1sGwNtE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json \n",
        "with open(\"na_prob.json\", \"w\") as outfile:\n",
        "    json.dump(na_prob_dict, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T11:27:19.683557Z",
          "iopub.status.busy": "2022-03-13T11:27:19.683291Z",
          "iopub.status.idle": "2022-03-13T11:27:19.696016Z",
          "shell.execute_reply": "2022-03-13T11:27:19.695219Z",
          "shell.execute_reply.started": "2022-03-13T11:27:19.683527Z"
        },
        "id": "6mWYMNMnwNtE",
        "outputId": "09822801-01f5-4d6e-9c96-3af52cdfcb5e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Which Lloyd Webber musical premiered in the US on 10th December 1993?\n",
            "Predicted answer: Cats\n",
            "Answers: []\n",
            "\n",
            "Question: Who was the next British Prime Minister after Arthur Balfour?\n",
            "Predicted answer: \n",
            "Answers: []\n",
            "\n",
            "Question: Who was the next British Prime Minister after Arthur Balfour?\n",
            "Predicted answer: \n",
            "Answers: []\n",
            "\n",
            "Question: Who had a 70s No 1 hit with Kiss You All Over?\n",
            "Predicted answer: Exile\n",
            "Answers: ['Exile']\n",
            "\n",
            "Question: What claimed the life of singer Kathleen Ferrier?\n",
            "Predicted answer: \n",
            "Answers: ['Cancer']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    print(f\"Question: {validation_dataset[i]['question']}\")\n",
        "    print(f\"Predicted answer: {pred_dict[validation_dataset[i]['id']]}\")\n",
        "    print(f\"Answers: {validation_dataset[i]['answers']['text']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2LYrUq9wNtE"
      },
      "source": [
        "Download the official evaluation script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T16:42:25.669697Z",
          "iopub.status.busy": "2022-03-13T16:42:25.668985Z",
          "iopub.status.idle": "2022-03-13T16:42:27.012666Z",
          "shell.execute_reply": "2022-03-13T16:42:27.011812Z",
          "shell.execute_reply.started": "2022-03-13T16:42:25.669649Z"
        },
        "id": "4A3OAAsZwNtE",
        "outputId": "b452f433-a404-462c-cf21-98be66190597",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "--2022-03-13 16:42:26--  https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
            "Resolving worksheets.codalab.org (worksheets.codalab.org)... 13.68.212.115\n",
            "Connecting to worksheets.codalab.org (worksheets.codalab.org)|13.68.212.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Syntax error in Set-Cookie: codalab_session=\"\"; expires=Thu, 01 Jan 1970 00:00:00 GMT; Max-Age=-1; Path=/ at position 70.\n",
            "Length: unspecified [text/x-python]\n",
            "Saving to: ‘evaluation.py’\n",
            "\n",
            "evaluation.py           [ <=>                ]  10.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-13 16:42:26 (121 MB/s) - ‘evaluation.py’ saved [10547]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/ -O evaluation.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2022-03-13T16:42:30.279786Z",
          "iopub.status.busy": "2022-03-13T16:42:30.279476Z",
          "iopub.status.idle": "2022-03-13T16:42:34.898509Z",
          "shell.execute_reply": "2022-03-13T16:42:34.897589Z",
          "shell.execute_reply.started": "2022-03-13T16:42:30.279753Z"
        },
        "id": "nWNaeH1KwNtF",
        "outputId": "0fbf0e94-4571-458e-e834-9378421ebd7f",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 37.739827113641155,\n",
            "  \"f1\": 44.6628950265895,\n",
            "  \"total\": 14229,\n",
            "  \"HasAns_exact\": 13.005486689697216,\n",
            "  \"HasAns_f1\": 23.014461830251516,\n",
            "  \"HasAns_total\": 9842,\n",
            "  \"NoAns_exact\": 93.22999772053795,\n",
            "  \"NoAns_f1\": 93.22999772053795,\n",
            "  \"NoAns_total\": 4387,\n",
            "  \"best_exact\": 37.753882915173236,\n",
            "  \"best_exact_thresh\": 0.37428009510040283,\n",
            "  \"best_f1\": 44.64415395788189,\n",
            "  \"best_f1_thresh\": 0.48292508721351624,\n",
            "  \"pr_exact_ap\": 3.52097137469337,\n",
            "  \"pr_f1_ap\": 9.97684824537978,\n",
            "  \"pr_oracle_ap\": 93.16980559099022\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!python evaluation.py \"../input/trivia-squadformat/triviaqa_dev (1).json\" pred.json --na-prob-file na_prob.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXlefjZWwNtF"
      },
      "source": [
        "# Evaluate trivia QA on SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "71fe382d20824ff99a063690fa5d8a63",
            "663e2111e03246c399b97d11d90a2d30",
            "f051d51b39174311b4f1ba475d50be0f",
            "14864e327edb44c0adabcad6fba36f2a",
            "6bec80c4b3974ce28052b2a236cbdf7c",
            "bfafea8d19b146bd91dae8b3595dab11",
            ""
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-13T16:42:55.320615Z",
          "iopub.status.busy": "2022-03-13T16:42:55.320313Z",
          "iopub.status.idle": "2022-03-13T16:43:09.192033Z",
          "shell.execute_reply": "2022-03-13T16:43:09.191271Z",
          "shell.execute_reply.started": "2022-03-13T16:42:55.320582Z"
        },
        "id": "CGgHvl-ewNtF",
        "outputId": "267b0afd-5971-4643-aaff-90f14659ef80",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71fe382d20824ff99a063690fa5d8a63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.87k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "663e2111e03246c399b97d11d90a2d30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f051d51b39174311b4f1ba475d50be0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14864e327edb44c0adabcad6fba36f2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/9.55M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bec80c4b3974ce28052b2a236cbdf7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/801k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfafea8d19b146bd91dae8b3595dab11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "validation_squad = load_dataset('squad_v2', split='validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T16:43:20.082108Z",
          "iopub.status.busy": "2022-03-13T16:43:20.081494Z",
          "iopub.status.idle": "2022-03-13T16:43:27.682624Z",
          "shell.execute_reply": "2022-03-13T16:43:27.681872Z",
          "shell.execute_reply.started": "2022-03-13T16:43:20.082060Z"
        },
        "id": "nfbaF11JwNtF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenized_validation_squad = tokenizer(validation_squad['context'], validation_squad['question'], truncation=True, padding=True, return_offsets_mapping=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T16:43:51.499069Z",
          "iopub.status.busy": "2022-03-13T16:43:51.498580Z",
          "iopub.status.idle": "2022-03-13T16:43:51.945294Z",
          "shell.execute_reply": "2022-03-13T16:43:51.944395Z",
          "shell.execute_reply.started": "2022-03-13T16:43:51.499030Z"
        },
        "id": "yoRpktTPwNtF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "val_data = TensorDataset(torch.tensor(tokenized_validation_squad['input_ids'], dtype=torch.int64),\n",
        "                        torch.tensor(tokenized_validation_squad['attention_mask'], dtype=torch.float))\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T16:43:55.944468Z",
          "iopub.status.busy": "2022-03-13T16:43:55.944051Z",
          "iopub.status.idle": "2022-03-13T17:09:07.787733Z",
          "shell.execute_reply": "2022-03-13T17:09:07.786968Z",
          "shell.execute_reply.started": "2022-03-13T16:43:55.944433Z"
        },
        "id": "aGQAkkBnwNtF",
        "outputId": "a414d2b3-a892-417b-cb70-7ea8e7963dfa",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1485/1485 [25:11<00:00,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy is:  0.3098627137202055\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"../input/question3-train/trivia2.h5\"))\n",
        "\n",
        "threshold = 1.0\n",
        "epoch_i = 0\n",
        "correct = 0 \n",
        "pred_dict = {}\n",
        "na_prob_dict = {}\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "batch_val_losses = []\n",
        "row = 0\n",
        "for test_batch in tqdm(val_dataloader):\n",
        "    input_ids, masks = tuple(t.to(device) for t in test_batch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # prediction logits\n",
        "        start_logits, end_logits = model(input_ids=input_ids,\n",
        "                                        attention_mask=masks,\n",
        "                                        return_dict=False)\n",
        "\n",
        "    # to cpu\n",
        "    start_logits = start_logits.detach().cpu()\n",
        "    end_logits = end_logits.detach().cpu()\n",
        "\n",
        "    # for every sequence in batch \n",
        "    for bidx in range(len(start_logits)):\n",
        "        # apply softmax to logits to get scores\n",
        "        start_scores = np.array(F.softmax(start_logits[bidx], dim = 0))\n",
        "        end_scores = np.array(F.softmax(end_logits[bidx], dim = 0))\n",
        "\n",
        "        # find max for start<=end\n",
        "        size = len(start_scores)\n",
        "        scores = np.zeros((size, size))\n",
        "        for j in range(size):\n",
        "            for i in range(j+1): # include j\n",
        "                scores[i,j] = start_scores[i] + end_scores[j]\n",
        "\n",
        "        # find best i and j\n",
        "        start_pred, end_pred = unravel_index(scores.argmax(), scores.shape)\n",
        "        answer_pred = \"\"\n",
        "        if (scores[start_pred, end_pred] > scores[0,0]+threshold):\n",
        "\n",
        "            offsets = tokenized_validation_squad.offset_mapping[row]\n",
        "            pred_char_start = offsets[start_pred][0]\n",
        "\n",
        "            if end_pred < len(offsets):\n",
        "                pred_char_end = offsets[end_pred][1]\n",
        "                answer_pred = validation_squad[row]['context'][pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                answer_pred = validation_squad[row]['context'][pred_char_start:]\n",
        "                \n",
        "            if answer_pred in validation_squad[row]['answers']['text']:\n",
        "                correct += 1\n",
        "\n",
        "        else:\n",
        "            if (len(validation_squad[row]['answers']['text']) ==0):\n",
        "                correct += 1        \n",
        "\n",
        "        pred_dict[validation_squad[row]['id']] = answer_pred\n",
        "        na_prob_dict[validation_squad[row]['id']] = scores[0,0]\n",
        "\n",
        "        row+=1\n",
        "\n",
        "\n",
        "accuracy = correct/validation_squad.num_rows\n",
        "print(\"accuracy is: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T17:11:48.480594Z",
          "iopub.status.busy": "2022-03-13T17:11:48.479863Z",
          "iopub.status.idle": "2022-03-13T17:11:48.511552Z",
          "shell.execute_reply": "2022-03-13T17:11:48.510812Z",
          "shell.execute_reply.started": "2022-03-13T17:11:48.480554Z"
        },
        "id": "9UtvYlQwwNtG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json \n",
        "with open(\"pred2.json\", \"w\") as outfile:\n",
        "    json.dump(pred_dict, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T17:11:49.671742Z",
          "iopub.status.busy": "2022-03-13T17:11:49.671476Z",
          "iopub.status.idle": "2022-03-13T17:11:49.722673Z",
          "shell.execute_reply": "2022-03-13T17:11:49.721884Z",
          "shell.execute_reply.started": "2022-03-13T17:11:49.671712Z"
        },
        "id": "lrpYxNdfwNtG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json \n",
        "with open(\"na_prob2.json\", \"w\") as outfile:\n",
        "    json.dump(na_prob_dict, outfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj4IgjrlwNtG"
      },
      "source": [
        "Get SQuAD dev.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T17:11:51.247463Z",
          "iopub.status.busy": "2022-03-13T17:11:51.247180Z",
          "iopub.status.idle": "2022-03-13T17:11:52.311260Z",
          "shell.execute_reply": "2022-03-13T17:11:52.310347Z",
          "shell.execute_reply.started": "2022-03-13T17:11:51.247432Z"
        },
        "id": "dK117xLPwNtG",
        "outputId": "8b54a274-ec11-41ac-86d3-287e01d0e8ac",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "--2022-03-13 17:11:51--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.111.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘squad_dev.json’\n",
            "\n",
            "squad_dev.json      100%[===================>]   4.17M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-03-13 17:11:52 (32.2 MB/s) - ‘squad_dev.json’ saved [4370528/4370528]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad_dev.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T17:12:04.501086Z",
          "iopub.status.busy": "2022-03-13T17:12:04.500738Z",
          "iopub.status.idle": "2022-03-13T17:12:09.418605Z",
          "shell.execute_reply": "2022-03-13T17:12:09.417725Z",
          "shell.execute_reply.started": "2022-03-13T17:12:04.501047Z"
        },
        "id": "3yjyMMtQwNtG",
        "outputId": "1c4ba63e-6f51-427f-a13a-964ba6a86c82",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 31.087341025856986,\n",
            "  \"f1\": 33.13962656169863,\n",
            "  \"total\": 11873,\n",
            "  \"HasAns_exact\": 6.275303643724697,\n",
            "  \"HasAns_f1\": 10.385760149637195,\n",
            "  \"HasAns_total\": 5928,\n",
            "  \"NoAns_exact\": 55.82842724978974,\n",
            "  \"NoAns_f1\": 55.82842724978974,\n",
            "  \"NoAns_total\": 5945,\n",
            "  \"best_exact\": 50.07159100480081,\n",
            "  \"best_exact_thresh\": 0.0,\n",
            "  \"best_f1\": 50.077205985569506,\n",
            "  \"best_f1_thresh\": 3.3368636650266126e-05,\n",
            "  \"pr_exact_ap\": 0.5230665528554378,\n",
            "  \"pr_f1_ap\": 1.4343411559985633,\n",
            "  \"pr_oracle_ap\": 51.01587119837339\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!python evaluation.py squad_dev.json pred2.json --na-prob-file na_prob2.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcj3z8I2wNtG"
      },
      "source": [
        "# Evaluate squad on squad (distilbert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-xKLrVtwNtG"
      },
      "source": [
        "First, fine tune squad with distilbert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T17:49:02.033432Z",
          "iopub.status.busy": "2022-03-13T17:49:02.033173Z",
          "iopub.status.idle": "2022-03-13T17:49:02.329572Z",
          "shell.execute_reply": "2022-03-13T17:49:02.328803Z",
          "shell.execute_reply.started": "2022-03-13T17:49:02.033403Z"
        },
        "id": "1nSfwz-EwNtH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "train_squad = load_dataset('squad_v2', split='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e37d2138a3474401a46c246123389fb4"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-13T17:49:04.304707Z",
          "iopub.status.busy": "2022-03-13T17:49:04.304255Z",
          "iopub.status.idle": "2022-03-13T17:50:39.246449Z",
          "shell.execute_reply": "2022-03-13T17:50:39.245701Z",
          "shell.execute_reply.started": "2022-03-13T17:49:04.304672Z"
        },
        "id": "lalwy2-9wNtH",
        "outputId": "c719d294-c12e-49be-f1a1-952faaf8af94",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e37d2138a3474401a46c246123389fb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/131 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_squad = train_squad.map(find_end)\n",
        "\n",
        "def encode(examples):\n",
        "    tokenized = tokenizer(examples['context'], examples['question'], truncation=True, padding='max_length', max_length=512)\n",
        "    answers = examples['answers']\n",
        "    start_token_list = []\n",
        "    end_token_list = []\n",
        "    for i in range(len(answers)):\n",
        "        if (answers[i]['text'] != ''):\n",
        "            start_token = tokenized.char_to_token(i, answers[i]['answer_start'])\n",
        "            end_token = tokenized.char_to_token(i, answers[i]['answer_end'] - 1)\n",
        "            \n",
        "            # if start token is None, the answer passage has been truncated\n",
        "            if start_token is None:\n",
        "                start_token = tokenizer.model_max_length\n",
        "            if end_token is None:\n",
        "                end_token = tokenizer.model_max_length\n",
        "        else:\n",
        "            start_token = 0\n",
        "            end_token = 0\n",
        "            \n",
        "        start_token_list.append(start_token)\n",
        "        end_token_list.append(end_token)\n",
        "            \n",
        "\n",
        "    return {\"start_position\": start_token_list, \"end_position\": end_token_list, \"input_ids\": tokenized['input_ids'], \"attention_mask\": tokenized['attention_mask']}\n",
        "\n",
        "train_squad = train_squad.map(encode, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T17:51:15.598076Z",
          "iopub.status.busy": "2022-03-13T17:51:15.597790Z",
          "iopub.status.idle": "2022-03-13T17:52:32.625964Z",
          "shell.execute_reply": "2022-03-13T17:52:32.625159Z",
          "shell.execute_reply.started": "2022-03-13T17:51:15.598038Z"
        },
        "id": "tIFTseEvwNtI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "train_data = TensorDataset(torch.tensor(train_squad['input_ids'], dtype=torch.int64), \n",
        "                           torch.tensor(train_squad['attention_mask'], dtype=torch.float), \n",
        "                           torch.tensor(train_squad['start_position'], dtype=torch.int64), \n",
        "                           torch.tensor(train_squad['start_position'], dtype=torch.int64))\n",
        "\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T17:58:00.993355Z",
          "iopub.status.busy": "2022-03-13T17:58:00.992784Z",
          "iopub.status.idle": "2022-03-13T17:58:02.190937Z",
          "shell.execute_reply": "2022-03-13T17:58:02.190176Z",
          "shell.execute_reply.started": "2022-03-13T17:58:00.993316Z"
        },
        "id": "EQEm6OviwNtI",
        "outputId": "5bc7a4c7-0767-4f4b-a388-9e939af35ce1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
        "epochs = 3\n",
        "model.to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T17:58:03.586195Z",
          "iopub.status.busy": "2022-03-13T17:58:03.585258Z",
          "iopub.status.idle": "2022-03-13T17:58:29.972205Z",
          "shell.execute_reply": "2022-03-13T17:58:29.970997Z",
          "shell.execute_reply.started": "2022-03-13T17:58:03.586142Z"
        },
        "id": "II_wyKPjwNtI",
        "outputId": "b912ebfe-e0f8-43a1-d789-0cbd02936616",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   1%|          | 109/16290 [00:26<1:05:12,  4.14it/s, Loss=3.73]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_1702/3791534224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "# model.load_state_dict(torch.load(\"./weights_\" + str(0) + \".pth\"))\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = []\n",
        "    validation_loss = []\n",
        "    \n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    count=-1\n",
        "    progress_bar = tqdm(train_dataloader, leave=True, position=0)\n",
        "    progress_bar.set_description(f\"Epoch {epoch+1}\")\n",
        "    for batch in progress_bar:\n",
        "        count+=1\n",
        "        input_ids, mask, start, end  = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss, start_logits, end_logits = model(input_ids = input_ids, \n",
        "                                                attention_mask = mask, \n",
        "                                                start_positions = start, \n",
        "                                                end_positions = end,\n",
        "                                                return_dict = False)           \n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (count % 20 == 0 and count != 0):\n",
        "            avg = total_loss/count\n",
        "            progress_bar.set_postfix(Loss=avg)\n",
        "            \n",
        "    torch.save(model.state_dict(), \"./squad\" + str(epoch) + \".h5\") # save for later use\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    epoch_loss.append(avg_train_loss)\n",
        "    print(f\"Epoch {epoch} Loss: {avg_train_loss}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVBDmAPOwNtI"
      },
      "outputs": [],
      "source": [
        "tokenized_validation = tokenizer(validation_squad['context'], validation_squad['question'], truncation=True, padding=True, return_offsets_mapping=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t_TiwikwNtI"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "val_data = TensorDataset(torch.tensor(tokenized_validation['input_ids'], dtype=torch.int64), \n",
        "                        torch.tensor(tokenized_validation['attention_mask'], dtype=torch.float))\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gn8Mq3iwNtI"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"./squad2.h5\"))\n",
        "\n",
        "threshold = 1.0\n",
        "epoch_i = 0\n",
        "correct = 0 \n",
        "pred_dict = {}\n",
        "na_prob_dict = {}\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "batch_val_losses = []\n",
        "row = 0\n",
        "for test_batch in tqdm(val_dataloader):\n",
        "    input_ids, masks = tuple(t.to(device) for t in test_batch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # prediction logits\n",
        "        start_logits, end_logits = model(input_ids=input_ids,\n",
        "                                        attention_mask=masks,\n",
        "                                        return_dict=False)\n",
        "\n",
        "    # to cpu\n",
        "    start_logits = start_logits.detach().cpu()\n",
        "    end_logits = end_logits.detach().cpu()\n",
        "\n",
        "    # for every sequence in batch \n",
        "    for bidx in range(len(start_logits)):\n",
        "        # apply softmax to logits to get scores\n",
        "        start_scores = np.array(F.softmax(start_logits[bidx], dim = 0))\n",
        "        end_scores = np.array(F.softmax(end_logits[bidx], dim = 0))\n",
        "\n",
        "        # find max for start<=end\n",
        "        size = len(start_scores)\n",
        "        scores = np.zeros((size, size))\n",
        "        for j in range(size):\n",
        "            for i in range(j+1): # include j\n",
        "                scores[i,j] = start_scores[i] + end_scores[j]\n",
        "\n",
        "        # find best i and j\n",
        "        start_pred, end_pred = unravel_index(scores.argmax(), scores.shape)\n",
        "        answer_pred = \"\"\n",
        "        if (scores[start_pred, end_pred] > scores[0,0]+threshold):\n",
        "\n",
        "            offsets = tokenized_validation_squad.offset_mapping[row]\n",
        "            pred_char_start = offsets[start_pred][0]\n",
        "\n",
        "            if end_pred < len(offsets):\n",
        "                pred_char_end = offsets[end_pred][1]\n",
        "                answer_pred = validation_squad[row]['context'][pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                answer_pred = validation_squad[row]['context'][pred_char_start:]\n",
        "                \n",
        "            if answer_pred in validation_squad[row]['answers']['text']:\n",
        "                correct += 1\n",
        "\n",
        "        else:\n",
        "            if (len(validation_squad[row]['answers']['text']) ==0):\n",
        "                correct += 1        \n",
        "\n",
        "        pred_dict[validation_squad[row]['id']] = answer_pred\n",
        "        na_prob_dict[validation_squad[row]['id']] = scores[0,0]\n",
        "\n",
        "        row+=1\n",
        "\n",
        "\n",
        "accuracy = correct/validation_squad.num_rows\n",
        "print(\"accuracy is: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A4ta-PrwNtJ"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "with open(\"pred3.json\", \"w\") as outfile:\n",
        "    json.dump(pred_dict, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLhP1xjfwNtJ"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "with open(\"na_prob3.json\", \"w\") as outfile:\n",
        "    json.dump(na_prob_dict, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KKXdlQPwNtJ"
      },
      "outputs": [],
      "source": [
        "!python evaluation.py squad_dev.json pred3.json --na-prob-file na_prob3.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmLrkliVwNtJ"
      },
      "source": [
        "# Evaluate squad on trivia QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "69b7a176a2ec443aaa5e609ad194f61b"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-13T17:59:04.742385Z",
          "iopub.status.busy": "2022-03-13T17:59:04.741819Z",
          "iopub.status.idle": "2022-03-13T18:02:48.330027Z",
          "shell.execute_reply": "2022-03-13T18:02:48.329242Z",
          "shell.execute_reply.started": "2022-03-13T17:59:04.742348Z"
        },
        "id": "AsYizex9wNtJ",
        "outputId": "87ce51f5-d2af-4296-be9d-4c087b427f85",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69b7a176a2ec443aaa5e609ad194f61b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "validation_dataset = validation_dataset.map(lambda examples: tokenizer(examples['context'], examples['question'], truncation=True, padding=True, return_offsets_mapping=True), batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T18:02:48.332333Z",
          "iopub.status.busy": "2022-03-13T18:02:48.331992Z",
          "iopub.status.idle": "2022-03-13T18:02:56.552877Z",
          "shell.execute_reply": "2022-03-13T18:02:56.552081Z",
          "shell.execute_reply.started": "2022-03-13T18:02:48.332293Z"
        },
        "id": "bgAsFSyPwNtJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "val_data = TensorDataset(torch.tensor(validation_dataset['input_ids'], dtype=torch.int64), \n",
        "                        torch.tensor(validation_dataset['attention_mask'], dtype=torch.float))\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T18:14:31.743659Z",
          "iopub.status.busy": "2022-03-13T18:14:31.743395Z",
          "iopub.status.idle": "2022-03-13T18:48:17.418811Z",
          "shell.execute_reply": "2022-03-13T18:48:17.417995Z",
          "shell.execute_reply.started": "2022-03-13T18:14:31.743629Z"
        },
        "id": "cU-A8WGowNtJ",
        "outputId": "43b0b1f1-50ce-492d-83f4-58cb923032a3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 1779/1779 [33:43<00:00,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy is:  0.30852484362920796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "model.load_state_dict(torch.load(\"../input/bert-weights/weights_2.h5\"))\n",
        "model.to(device)\n",
        "\n",
        "threshold = 1.0\n",
        "epoch_i = 0\n",
        "correct = 0 \n",
        "pred_dict = {}\n",
        "na_prob_dict = {}\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "batch_val_losses = []\n",
        "row = 0\n",
        "for test_batch in tqdm(val_dataloader):\n",
        "    input_ids, masks = tuple(t.to(device) for t in test_batch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # prediction logits\n",
        "        start_logits, end_logits = model(input_ids=input_ids,\n",
        "                                        attention_mask=masks,\n",
        "                                        return_dict=False)\n",
        "\n",
        "    # to cpu\n",
        "    start_logits = start_logits.detach().cpu()\n",
        "    end_logits = end_logits.detach().cpu()\n",
        "\n",
        "    # for every sequence in batch \n",
        "    for bidx in range(len(start_logits)):\n",
        "        # apply softmax to logits to get scores\n",
        "        start_scores = np.array(F.softmax(start_logits[bidx], dim = 0))\n",
        "        end_scores = np.array(F.softmax(end_logits[bidx], dim = 0))\n",
        "\n",
        "        # find max for start<=end\n",
        "        size = len(start_scores)\n",
        "        scores = np.zeros((size, size))\n",
        "        for j in range(size):\n",
        "            for i in range(j+1): # include j\n",
        "                scores[i,j] = start_scores[i] + end_scores[j]\n",
        "\n",
        "        # find best i and j\n",
        "        start_pred, end_pred = unravel_index(scores.argmax(), scores.shape)\n",
        "        answer_pred = \"\"\n",
        "        if (scores[start_pred, end_pred] > scores[0,0]+threshold):\n",
        "\n",
        "            offsets = validation_dataset[row]['offset_mapping']\n",
        "            pred_char_start = offsets[start_pred][0]\n",
        "\n",
        "            if end_pred < len(offsets):\n",
        "                pred_char_end = offsets[end_pred][1]\n",
        "                answer_pred = validation_dataset[row]['context'][pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                answer_pred = validation_dataset[row]['context'][pred_char_start:]\n",
        "                \n",
        "            if answer_pred in validation_dataset[row]['answers']['text']:\n",
        "                correct += 1\n",
        "\n",
        "        else:\n",
        "            if (len(validation_dataset[row]['answers']['text']) ==0):\n",
        "                correct += 1        \n",
        "\n",
        "        pred_dict[validation_dataset[row]['id']] = answer_pred\n",
        "        na_prob_dict[validation_dataset[row]['id']] = scores[0,0]\n",
        "\n",
        "        row+=1\n",
        "\n",
        "\n",
        "accuracy = correct/validation_dataset.num_rows\n",
        "print(\"accuracy is: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T18:59:59.837413Z",
          "iopub.status.busy": "2022-03-13T18:59:59.836620Z",
          "iopub.status.idle": "2022-03-13T18:59:59.874723Z",
          "shell.execute_reply": "2022-03-13T18:59:59.873986Z",
          "shell.execute_reply.started": "2022-03-13T18:59:59.837374Z"
        },
        "id": "BWhcWk-zwNtJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json \n",
        "with open(\"pred4.json\", \"w\") as outfile:\n",
        "    json.dump(pred_dict, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T19:00:01.385575Z",
          "iopub.status.busy": "2022-03-13T19:00:01.385053Z",
          "iopub.status.idle": "2022-03-13T19:00:01.445002Z",
          "shell.execute_reply": "2022-03-13T19:00:01.444163Z",
          "shell.execute_reply.started": "2022-03-13T19:00:01.385538Z"
        },
        "id": "oK8J42YpwNtK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json \n",
        "with open(\"na_prob4.json\", \"w\") as outfile:\n",
        "    json.dump(na_prob_dict, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T19:00:02.995090Z",
          "iopub.status.busy": "2022-03-13T19:00:02.994298Z",
          "iopub.status.idle": "2022-03-13T19:00:08.168886Z",
          "shell.execute_reply": "2022-03-13T19:00:08.167892Z",
          "shell.execute_reply.started": "2022-03-13T19:00:02.994990Z"
        },
        "id": "kyFI790pwNtK",
        "outputId": "bf311e44-353c-454c-858b-506e1f6e170e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 30.852484362920794,\n",
            "  \"f1\": 30.858106683533627,\n",
            "  \"total\": 14229,\n",
            "  \"HasAns_exact\": 0.0406421459053038,\n",
            "  \"HasAns_f1\": 0.04877057508636456,\n",
            "  \"HasAns_total\": 9842,\n",
            "  \"NoAns_exact\": 99.97720537953043,\n",
            "  \"NoAns_f1\": 99.97720537953043,\n",
            "  \"NoAns_total\": 4387,\n",
            "  \"best_exact\": 30.852484362920794,\n",
            "  \"best_exact_thresh\": 0.09740130603313446,\n",
            "  \"best_f1\": 30.858106683533627,\n",
            "  \"best_f1_thresh\": 0.09740130603313446,\n",
            "  \"pr_exact_ap\": 0.00035310544329806957,\n",
            "  \"pr_f1_ap\": 0.0004063443578802724,\n",
            "  \"pr_oracle_ap\": 67.37022560958722\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!python evaluation.py \"../input/trivia-squadformat/triviaqa_dev (1).json\" pred4.json --na-prob-file na_prob4.json"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "question3_teliko.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
